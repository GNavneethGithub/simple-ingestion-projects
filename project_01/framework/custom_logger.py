import datetime
import inspect
import json
import logging
import os

class CustomLogFormatter(logging.Formatter):
    """
    A custom logging formatter designed to produce structured, multi-line log
    output, incorporating custom attributes such as parent/child IDs, a keyword,
    and additional details.

    This formatter extends `logging.Formatter` to allow for a highly
    customizable log message structure, making logs easier to read and parse
    for specific information.
    """
    def format(self, record: logging.LogRecord) -> str:
        """
        Formats the specified log record as a string.

        Args:
            record (logging.LogRecord): The log record to format.

        Returns:
            str: The formatted log message string.
        """
        # Retrieve custom attributes from the log record's 'extra' dictionary
        parent_id = getattr(record, 'parent_id', "N/A")
        child_id = getattr(record, 'child_id', "N/A")
        keyword = getattr(record, 'keyword', "")
        other_details = getattr(record, 'other_details', None)
        caller_info = getattr(record, 'caller_info', "N/A:N/A")

        # Format timestamp to include milliseconds
        log_timestamp = datetime.datetime.fromtimestamp(record.created).strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]

        # Format the keyword for display, if provided
        keyword_display = f"[{keyword.strip()}] " if keyword.strip() else ""

        # Retrieve the main message content from the record
        message_content = record.getMessage() if record.getMessage() else ""

        # Format 'other_details' dictionary into a readable JSON string, if present
        other_details_display = ""
        if other_details is not None:
            try:
                # Use json.dumps for pretty printing with 4-space indentation
                other_details_display = f"\nOther Details:\n{json.dumps(other_details, indent=4)}"
            except TypeError:
                # Fallback for non-JSON serializable objects in other_details
                other_details_display = f"\nOther Details:\n{other_details}"

        # Construct the final multi-line log output string
        formatted_log_output = f"""-----------------------------------------------------------------------
{keyword_display}{record.levelname}
Parent ID: {parent_id} | Child ID: {child_id}
Log Time: {log_timestamp} | Caller: {caller_info}
Message:
{message_content}{other_details_display}
-----------------------------------------------------------------------
"""
        return formatted_log_output


class CustomLogger:
    """
    A wrapper class for Python's built-in `logging.Logger` module, providing
    a custom, structured logging interface.

    This class facilitates logging with additional contextual information such as
    parent and child IDs, a searchable keyword, and a dictionary for arbitrary
    structured details. It ensures that logs are formatted consistently and
    can be easily integrated into standard logging pipelines.
    """

    def __init__(self, parent_id: str = "N/A", child_id: str = "N/A", name: str = "custom_logger_instance",
                 level: int = logging.INFO, log_file: str | None = None):
        """
        Initializes a new instance of CustomLogger.

        Configures an underlying `logging.Logger` with a `StreamHandler` and
        `CustomLogFormatter` to output logs to the console. Optionally, a
        `FileHandler` can be added to write logs to a specified file.

        Args:
            parent_id (str): The default parent identifier for log entries
                             generated by this logger instance. Defaults to "N/A".
            child_id (str): The default child identifier for log entries
                            generated by this logger instance. Defaults to "N/A".
            name (str): A unique name for the underlying `logging.Logger` instance.
                        This is crucial for managing loggers, especially in
                        multiprocessing environments, to prevent duplicate handlers.
                        Defaults to "custom_logger_instance".
            level (int): The minimum logging level for this logger instance.
                         Messages with a severity lower than this level will be ignored.
                         Defaults to `logging.INFO`.
            log_file (str | None): Optional path to a log file. If provided, a
                                   `FileHandler` will be added to write logs
                                   to this file. Defaults to `None`.
        """
        self._instance_parent_id: str = parent_id
        self._instance_child_id: str = child_id

        # Retrieve or create the underlying Python logger instance by name
        self._logger: logging.Logger = logging.getLogger(name)
        # Set the minimum logging level for this logger instance
        self._logger.setLevel(level)

        # Configure handlers only if no handlers are already attached to this logger.
        # This prevents duplicate log output, especially important when a logger
        # is retrieved by name multiple times or in child processes.
        if not self._logger.handlers:
            # Create a console handler to output logs to stderr (default for StreamHandler)
            console_handler = logging.StreamHandler()
            # Instantiate the custom formatter
            formatter = CustomLogFormatter()
            # Assign the custom formatter to the handler
            console_handler.setFormatter(formatter)
            # Add the configured handler to the logger
            self._logger.addHandler(console_handler)

            # If a log file is specified, add a FileHandler
            if log_file:
                file_handler = logging.FileHandler(log_file)
                file_handler.setFormatter(formatter) # Use the same custom formatter
                self._logger.addHandler(file_handler)

            # Optional: Disable propagation to the root logger to avoid duplicate
            # output if the root logger also has handlers configured.
            # self._logger.propagate = False

    def _get_caller_info(self) -> str:
        """
        Retrieves the file name and line number of the code that invoked
        the public logging method (e.g., `info`, `error`).

        This helps in tracing the origin of a log message within the codebase.

        Returns:
            str: A string in the format "filename:line_number".
        """
        # inspect.stack() returns a list of frame records.
        # [0] is this current frame (_get_caller_info)
        # [1] is the _log method
        # [2] is the public method (debug, info, etc.)
        # [3] is the actual user code calling debug/info/etc.
        caller_frame = inspect.stack()[3]
        # Extract just the filename from the full path
        file_name = os.path.basename(caller_frame.filename)
        line_number = caller_frame.lineno
        return f"{file_name}:{line_number}"

    def _log(self, level: int, message: str | None = None, parent_id: str | None = None, child_id: str | None = None, keyword: str = "", other_details: dict | None = None):
        """
        Internal method to dispatch the log message to the underlying `logging.Logger`.

        This method prepares the custom log attributes and passes them to the
        `logging.Logger` via the `extra` dictionary, allowing the `CustomLogFormatter`
        to access and format them.

        Args:
            level (int): The standard logging level (e.g., `logging.INFO`, `logging.ERROR`).
            message (str | None): The primary log message. Can be multi-line or `None`.
            parent_id (str | None): Overrides the instance's default parent ID for this
                                     specific log entry. If `None`, the instance's
                                     `_instance_parent_id` is used.
            child_id (str | None): Overrides the instance's default child ID for this
                                    specific log entry. If `None`, the instance's
                                    `_instance_child_id` is used.
            keyword (str): An optional, easy-to-search keyword for the log entry.
            other_details (dict | None): An optional dictionary containing additional
                                         structured details for the log entry.
        """
        # Determine the effective parent and child IDs for the current log record
        effective_parent_id = parent_id if parent_id is not None else self._instance_parent_id
        effective_child_id = child_id if child_id is not None else self._instance_child_id

        # Construct the `extra` dictionary to pass custom attributes to the formatter
        extra_attributes = {
            'parent_id': effective_parent_id,
            'child_id': effective_child_id,
            'keyword': keyword,
            'other_details': other_details,
            'caller_info': self._get_caller_info() # Dynamically get caller info for each log
        }

        # Dispatch the log message using the underlying logging.Logger
        # The message can be an empty string if None is passed, as logging.Logger expects a string.
        self._logger.log(level, message if message is not None else "", extra=extra_attributes)

    def debug(self, message: str | None = None, parent_id: str | None = None, child_id: str | None = None, keyword: str = "", other_details: dict | None = None):
        """
        Logs a message with the DEBUG level.

        Args:
            message (str | None): The primary log message.
            parent_id (str | None): Overrides the instance's default parent ID.
            child_id (str | None): Overrides the instance's default child ID.
            keyword (str): An optional, searchable keyword.
            other_details (dict | None): Optional structured details.
        """
        self._log(logging.DEBUG, message, parent_id, child_id, keyword, other_details)

    def info(self, message: str | None = None, parent_id: str | None = None, child_id: str | None = None, keyword: str = "", other_details: dict | None = None):
        """
        Logs a message with the INFO level.

        Args:
            message (str | None): The primary log message.
            parent_id (str | None): Overrides the instance's default parent ID.
            child_id (str | None): Overrides the instance's default child ID.
            keyword (str): An optional, searchable keyword.
            other_details (dict | None): Optional structured details.
        """
        self._log(logging.INFO, message, parent_id, child_id, keyword, other_details)

    def warning(self, message: str | None = None, parent_id: str | None = None, child_id: str | None = None, keyword: str = "", other_details: dict | None = None):
        """
        Logs a message with the WARNING level.

        Args:
            message (str | None): The primary log message.
            parent_id (str | None): Overrides the instance's default parent ID.
            child_id (str | None): Overrides the instance's default child ID.
            keyword (str): An optional, searchable keyword.
            other_details (dict | None): Optional structured details.
        """
        self._log(logging.WARNING, message, parent_id, child_id, keyword, other_details)

    def error(self, message: str | None = None, parent_id: str | None = None, child_id: str | None = None, keyword: str = "", other_details: dict | None = None):
        """
        Logs a message with the ERROR level.

        Args:
            message (str | None): The primary log message.
            parent_id (str | None): Overrides the instance's default parent ID.
            child_id (str | None): Overrides the instance's default child ID.
            keyword (str): An optional, searchable keyword.
            other_details (dict | None): Optional structured details.
        """
        self._log(logging.ERROR, message, parent_id, child_id, keyword, other_details)

    def critical(self, message: str | None = None, parent_id: str | None = None, child_id: str | None = None, keyword: str = "", other_details: dict | None = None):
        """
        Logs a message with the CRITICAL level.

        Args:
            message (str | None): The primary log message.
            parent_id (str | None): Overrides the instance's default parent ID.
            child_id (str | None): Overrides the instance's default child ID.
            keyword (str): An optional, searchable keyword.
            other_details (dict | None): Optional structured details.
        """
        self._log(logging.CRITICAL, message, parent_id, child_id, keyword, other_details)



# import multiprocessing
# # --- Example Usage ---
# if __name__ == "__main__":
#     # --- Global Logging Configuration (Optional but Recommended for Production) ---
#     # It's good practice to configure the root logger or disable propagation
#     # if you want your custom loggers to be entirely independent.
#     # For this example, CustomLogger instances manage their own handlers,
#     # so explicit root logger config might not be strictly needed unless
#     # other parts of your app use the root logger.
#     # logging.basicConfig(level=logging.WARNING) # Example: Set root logger to WARNING

#     # --- Example 1: Main Pipeline Log with INFO level and File Output ---
#     # Logs will go to console and 'main_pipeline.log'
#     main_logger = CustomLogger(
#         parent_id="main_pipeline_execution",
#         name="main_application_logger",
#         level=logging.INFO, # Only INFO and above will be logged by this instance
#         log_file="main_pipeline.log"
#     )

#     print("--- Example 1: Main Pipeline Log (INFO level, Console + File) ---")
#     main_logger.info(
#         "Application startup sequence initiated.",
#         keyword="APP_INIT",
#         other_details={"version": "1.0.0", "environment": "production"}
#     )
#     main_logger.debug(
#         "This DEBUG message will NOT appear because logger level is INFO.",
#         keyword="DEBUG_TEST"
#     )
#     main_logger.warning(
#         "Configuration check found minor discrepancies.",
#         keyword="CONFIG_WARN",
#         other_details={"config_file": "app_settings.json"}
#     )


#     def sub_function_01(config: dict | None, parent_logger_instance: CustomLogger):
#         """
#         Simulates a sub-function within the main pipeline, demonstrating
#         how to manage logging context for child operations.
#         """
#         print("\n--- Inside sub_function_01 ---")
#         # Option 1: Create a new CustomLogger instance for the child context.
#         # This is ideal when a distinct logging context (e.g., unique child_id)
#         # is needed for a specific logical unit of work within a function.
#         # It's crucial to provide a unique `name` to the underlying `logging.Logger`
#         # to prevent handler duplication, especially in multiprocessing.
#         # This child logger will also write to its own file.
#         child_logger_for_sub_task = CustomLogger(
#             parent_id=parent_logger_instance._instance_parent_id, # Inherit parent_id from the main logger
#             child_id="data_ingestion_phase_01", # Assign a unique child ID for this sub-task
#             name="sub_function_01_ingestion_logger", # Unique name for this logger instance
#             level=logging.DEBUG, # This child logger will capture DEBUG messages
#             log_file="sub_function_01.log"
#         )
#         child_logger_for_sub_task.info(
#             keyword='DATA_INGESTION',
#             message='Commencing data chunk processing.',
#             other_details={"chunk_identifier": "chunk_A_001", "status": "in_progress"}
#         )
#         child_logger_for_sub_task.debug(
#             "Detailed debug info during chunk processing.",
#             keyword="CHUNK_DEBUG",
#             other_details={"raw_data_size": 10240}
#         )
#         child_logger_for_sub_task.warning(
#             "Minor data inconsistencies detected during pre-processing.",
#             keyword="DATA_WARNING",
#             other_details={"inconsistency_count": 2, "affected_records": [12, 45]}
#         )

#         # Option 2: Use the passed parent logger instance and override child_id
#         # for specific log entries. This is suitable when most logs in a function
#         # share the parent's context, but a few specific logs need a temporary
#         # child identifier.
#         parent_logger_instance.debug(
#             "Debugging network connection within sub_function_01 using parent logger.",
#             child_id="network_check_override", # Temporarily override child_id for this log
#             keyword="NETWORK_DEBUG"
#         ) # This debug message will NOT appear if main_logger's level is INFO


#     # Call the simulated sub-function
#     sub_function_01(None, main_logger)

#     print("\n--- Back in Main Context ---")
#     main_logger.info("Main pipeline execution resuming after sub-function completion.", keyword="MAIN_RESUME")

#     # --- Example 2: Standalone Child Process Logger with ERROR level ---
#     # This logger will only show ERROR and CRITICAL messages to console.
#     print("\n--- Example 2: Standalone Child Process Logger (ERROR level) ---")
#     standalone_child_process_logger = CustomLogger(
#         child_id="standalone_worker_process_XYZ",
#         name="standalone_worker_logger",
#         level=logging.ERROR # Only ERROR and CRITICAL will be logged
#     )
#     standalone_child_process_logger.info(
#         "This INFO message will NOT appear from standalone logger.",
#         keyword="INFO_TEST"
#     )
#     standalone_child_process_logger.error(
#         "Standalone worker process encountered a critical error.",
#         keyword="WORKER_CRITICAL_ERROR",
#         other_details={"process_name": "data_cleaner_service", "exit_code": 1}
#     )
#     standalone_child_process_logger.critical(
#         "FATAL: Standalone worker process terminated unexpectedly.",
#         keyword="WORKER_FATAL"
#     )

#     # --- Example 3: Parallelization Testing with Multiprocessing ---
#     print("\n--- Example 3: Parallel Processing with Custom Logging ---")

#     def worker_function(task_id: int, common_parent_id: str):
#         """
#         A function executed by a separate process, demonstrating custom logging
#         within a multiprocessing context. Each worker creates its own logger
#         instance to ensure proper isolation and context.
#         """
#         # Create a CustomLogger instance for this specific worker process.
#         # It's crucial to give a unique name to the underlying logging.Logger
#         # within each process to prevent handler conflicts and ensure logs are
#         # properly captured by the process's own handlers.
#         worker_process_logger = CustomLogger(
#             parent_id=common_parent_id, # Inherit the parent ID from the main process
#             child_id=f"worker_task_{task_id}_pid_{os.getpid()}", # Unique child ID including process ID
#             name=f"worker_logger_task_{task_id}", # Unique logger name for this process
#             level=logging.DEBUG, # Workers will log DEBUG messages
#             log_file=f"worker_{task_id}.log" # Each worker logs to its own file
#         )
#         worker_process_logger.info(
#             f"Worker {task_id} initiated processing.",
#             keyword="WORKER_START",
#             other_details={"assigned_data_block": f"block_{task_id}", "process_id": os.getpid()}
#         )
#         try:
#             # Simulate work and potential errors within the worker
#             if task_id % 2 == 0:
#                 worker_process_logger.debug(
#                     f"Worker {task_id} successfully processed even-numbered task.",
#                     keyword="TASK_COMPLETION",
#                     other_details={"result_size_kb": 500}
#                 )
#             else:
#                 # Simulate an error for odd-numbered tasks
#                 raise ValueError(f"Simulated processing error for task {task_id}")
#         except ValueError as e:
#             worker_process_logger.error(
#                 f"Worker {task_id} failed to complete its assigned task.",
#                 keyword="WORKER_TASK_FAILURE",
#                 other_details={"exception_type": type(e).__name__, "error_message": str(e)}
#             )
#         finally:
#             worker_process_logger.info(
#                 f"Worker {task_id} concluded execution.",
#                 keyword="WORKER_END"
#             )

#     # Define the number of parallel processes to spawn
#     number_of_processes = 3
#     active_processes = []

#     # Create and start worker processes
#     for i in range(number_of_processes):
#         process = multiprocessing.Process(
#             target=worker_function,
#             args=(i + 1, main_logger._instance_parent_id) # Pass parent ID to workers
#         )
#         active_processes.append(process)
#         process.start()

#     # Wait for all child processes to complete their execution
#     for process in active_processes:
#         process.join()

#     main_logger.info("All parallel worker processes have successfully completed.", keyword="PARALLEL_EXECUTION_DONE")

#     # Clean up generated log files (optional, for demonstration purposes)
#     print("\n--- Cleaning up generated log files ---")
#     for filename in ["main_pipeline.log", "sub_function_01.log", "worker_1.log", "worker_2.log", "worker_3.log"]:
#         if os.path.exists(filename):
#             os.remove(filename)
#             print(f"Removed {filename}")





